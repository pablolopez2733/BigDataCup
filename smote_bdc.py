# -*- coding: utf-8 -*-
"""Smote_BDC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZKVcsJdZxlezGqKvtbHsahlY6HyWUQ7E
"""

# Commented out IPython magic to ensure Python compatibility.
#Import all necessary libraries
import pandas as pd
import numpy as np
import xgboost as xgb
from xgboost import XGBClassifier, plot_importance, plot_tree
import matplotlib.pyplot as plt
from matplotlib.pyplot import style
from sklearn.metrics import f1_score
import hyperopt
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline
from google.colab import drive
# %matplotlib inline
style.use('fivethirtyeight')

#Mouting Google Drive

drive.mount('/content/gdrive')

#Importing the data from Google Drive and showing the head of the data

path = '/content/gdrive/MyDrive/Experiments/BigDataCup/clean_data.csv'
df = pd.read_csv(path)
df.head()

#Counting Number of Instances of data per game

df.groupby('game_id')['game_id'].count()

#Calculating max goals for the home and away teams to later create binary classifier for win and loss

df['home_max'] = df['Home.Team.Goals'].groupby(df['game_id']).transform('max')
df['away_max'] = df['Away.Team.Goals'].groupby(df['game_id']).transform('max')
df.head()

#Creating second DF with needed features for win probability model

df2 = pd.DataFrame()
df2['game_id'] = df['game_id']
df2['home_team'] = df['Home.Team']
df2['away_team'] = df['Away.Team']
df2['off_team'] = df['Team']
df2['def_team'] = np.where(df['Team'] != df['Home.Team'], df['Home.Team'], df['Away.Team'])
df2['Time_Remaining'] = df['game_time_remaining']
df2['off_skaters'] = np.where(df['Team'] == df['Home.Team'],df['Home.Team.Skaters'],df['Away.Team.Skaters'])
df2['def_skaters'] = np.where(df['Team'] == df['Away.Team'],df['Away.Team.Skaters'],df['Home.Team.Skaters'])
df2['skater_diff'] = df2['off_skaters'] - df2['def_skaters']
df2['off_team_score'] = np.where(df['Team'] == df['Home.Team'], df['Home.Team.Goals'],df['Away.Team.Goals'])
df2['def_team_score'] = np.where(df['Team'] == df['Home.Team'], df['Away.Team.Goals'],df['Home.Team.Goals'])
df2['score_diff'] = df2['off_team_score'] - df2['def_team_score']
df2['xT'] = df['xT']
df2['oxT'] = df['oxT']
df2['nxT'] = df['nxT']
df2['off_team_final'] = np.where(df['Team'] == df['Home.Team'],df['home_max'],df['away_max'])
df2['def_team_final'] = np.where(df['Team'] != df['Home.Team'],df['home_max'],df['away_max'])
df2['Win'] = np.where(df2['off_team_final'] > df2['def_team_final'],1,0)
df2.head()

#Create DataFrame that will further be split for training and testing
#Columns that are dropped were needed for previous calculations but are not needed for training and testing the model

df3 = pd.DataFrame()
df3['game_id'] = df2['game_id']
df3['time_remaining(sec)'] = df2['Time_Remaining']
df3['skater_diff'] = df2['skater_diff']
df3['score_diff'] = df2['score_diff']
df3['nxT'] = df2['nxT']
df3['euclidean_dist'] = np.linalg.norm((df['start_x'].to_numpy(),df['start_y'].to_numpy)-(df['end_x'].to_numpy,df['end_y'].to_numpy))
df3['Win'] = df2['Win']
df3.head()

#Splitting Data into training and testing splits
#game_id #'s 14-39 are used as the training and 40-53 are used as testing for an ideal 70-30 train test split %

train_df = df3[(df3.game_id < 40)]
train_df = train_df.drop(['game_id'],axis=1)
test_df = df3[(df3.game_id >= 40)]
test_df = test_df.drop(['game_id'],axis=1)
train_df.head()

#Plotting the value counts of Majority and Minority classes showing that there is an imbalance of plays from winning vs. losing teams

plt.title("Value counts of Classes")
plt.xlabel('Classes')
plt.ylabel('Value Counts')
train_df.Win.value_counts().plot(kind='bar')

#Creating Balanced Data using SMOTE
#Creating X_train and Y_train variables to better oversample the data

X_train = train_df.iloc[:,0:4]
Y_train = train_df['Win']
sm = SMOTE(random_state=123)
X_train_res, Y_train_res = sm.fit_sample(X_train,Y_train)

#Converting numpy arrays to DataFrames

col_names = train_df.columns.to_list()
X_train_res = pd.DataFrame(X_train_res)
Y_train_res = pd.DataFrame(Y_train_res)
col_names2 = X_train_res.columns.to_list()
Y_train_res.head()

#Creating X and Y Dataframes for the testing data

X_test = test_df.iloc[:,0:4]
Y_test = test_df['Win']
X_test.columns = col_names2
X_test.head()

#Creating functions for parameter optimization and scoring with the Hyperopt package

def score(params):
    model = XGBClassifier(**params)
    
    model.fit(X_train_res, Y_train_res, eval_set=[(X_train_res, Y_train_res), (X_test, Y_test)],
              verbose=False, early_stopping_rounds=10)
    Y_pred = model.predict(X_test).clip(0, 20)
    score = (f1_score(Y_test, Y_pred))
    print(score)
    return {'loss': score, 'status': STATUS_OK}    
    
def optimize(trials, space):
    
    best = fmin(score, space, algo=tpe.suggest, max_evals=1000)
    return best

#Creating the scoring dictionary for the Hyperopt Parameter tuning

space = {
        'max_depth':hp.choice('max_depth', np.arange(1, 20, 1, dtype=int)),
        'n_estimators':hp.choice('n_estimators', np.arange(0, 10000, 1, dtype=int)),
        'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 1.0, 0.1),
        'min_child_weight':hp.choice('min_child_weight', np.arange(0, 120, 1, dtype=int)),
        'subsample':hp.quniform('subsample', 0.5, 1, 0.1),
        'eta':hp.quniform('eta', 0.1, 0.5, 0.1),
        'learning_rate':hp.choice('learning_rate',np.arange(.1,1.1,.1,dtype=float)),
        
        'objective':'binary:logistic',
    }

#Running n number of trials based on max_evals variable in the optimize function

trials = Trials()
best_params = optimize(trials, space)

# Return the best parameters
space_eval(space, best_params)

#Model tuned with best parameters

alg = XGBClassifier(colsample_bytree=0.9,learning_rate=0.1, max_depth=3)
algs = alg.fit(X_train_res,Y_train_res)
y_pred = algs.predict(X_test)
print("F1 Score: " + str(f1_score(Y_test,y_pred)))

y_pred = pd.DataFrame(alg.predict_proba(X_test))
y_pred.describe()

plot_importance(algs)

col_names
